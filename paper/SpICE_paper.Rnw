\documentclass[smallextended,natbib]{svjour3}  
\smartqed  % flush right qed marks, e.g. at end of proof

\usepackage{graphicx}
%\usepackage{array,multirow,graphicx}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{xcolor}
%\usepackage{url} % not crucial - just used below for the URL 
%\usepackage{multirow}
\usepackage{mathtools}
\usepackage{amsfonts}
%\usepackage{rotating}
%\usepackage{booktabs}
%\usepackage{verbatim}
%\usepackage{mathrsfs}
\usepackage{hyperref}
\usepackage{pdflscape}
%\usepackage{caption}
%\usepackage{tablefootnote}
\usepackage{verbatim}
\usepackage{setspace}
\usepackage[utf8]{inputenc}
%\usepackage{authblk}
%\usepackage{pdfpages}
%\usepackage{fancyhdr}
\usepackage{bbm}

\newcommand{\1}[1]{\mathbbm{1}_{#1}} 

% \parskip 2mm           
% \newcommand{\blind}{0}
% \newcommand{\blA}{\mbox{\boldmath {\bf A}}}
% \newcommand{\blB}{\mbox{\boldmath {\bf B}}}
% \newcommand{\blW}{\mbox{\boldmath {\bf W}}}
% \newcommand{\blX}{\mbox{\boldmath {\bf X}}}
% \newcommand{\blM}{\mbox{\boldmath {\bf M}}}
% \newcommand{\blx}{\mbox{\boldmath {\bf x}}}
% \newcommand{\bla}{\mbox{\boldmath {\bf a}}}
% \newcommand{\blalpha}{\mbox{\boldmath {\alpha}}}

\begin{document}

\title{SpICE: An interpretable method for spatial data} 

\author{Natalia da Silva \and Ignacio Alvarez-Castro\and
        Leonardo Moreno\and
        Andrés Sosa}

%\authorrunning{Short form of author list} % if too long for running head

\institute{Corresponding author: Natalia da Silva \at
              Instituto de Estadística (IESTA), Universidad de la República, Montevideo, Uruguay.\\
              \email{natalia.dasilva@fcea.edu.uy}\\
              ORCID:0000-0002-6031-7451\\
           \and
          Ignacio Alvarez-Castro \at 
          Instituto de Estadística (IESTA), Universidad de la República, Montevideo, Uruguay.\\
          \email{ignacio.alvarez@fcea.edu.uy}\\
          ORCID:0000-0003-1633-2432\\
       \and
          Leonardo Moreno \at
          Instituto de Estadística (IESTA), Universidad de la República, Montevideo, Uruguay.\\
          \email{leonardo.moreno@fcea.edu.uy}\\
          ORCID:0000-0003-1630-1361\\
             \and
             Andrés Sosa \at
          Instituto de Estadística (IESTA), Universidad de la República, Montevideo, Uruguay.\\
          \email{andres.sosa@fcea.edu.uy}\\
          ORCID:0000-0002-6007-4373\\
}


\date{Received:  / Accepted: }

\maketitle

\begin{abstract}
Statistical learning methods are widely utilised in tackling complex problems due to their flexibility, good predictive performance and ability to capture complex relationships among variables. Additionally, recently developed automatic workflows have provided a standardised approach for implementing statistical learning methods across various applications. However, these tools highlight one of the main drawbacks of statistical learning: the lack of interpretability of the results.
In the past few years, a large amount of research has been focused on methods for interpreting black box models.  Having interpretable statistical learning methods is necessary for obtaining a deeper understanding of these models. Specifically in problems in which spatial information is relevant, combining interpretable methods with spatial data can help to provide a better understanding of the problem and an improved interpretation of the results. 

This paper is focused on the individual conditional expectation plot (ICE-plot), a model-agnostic method for interpreting statistical learning models and combining them with spatial information. An ICE-plot extension is proposed in which spatial information is used as a restriction to define spatial ICE (SpICE) curves. Spatial ICE curves are estimated using real data in the context of an economic problem concerning property valuation in Montevideo, Uruguay. Understanding the key factors that influence property valuation is essential for decision-making, and spatial data play a relevant role in this regard. 

\keywords{Interpretable method \and spatial data \and statistical learning \and real estate }
\end{abstract}

\newpage

\section{Introduction \label{intro}}

Statistical learning methods have been used successfully in many fields and for different kinds of complex research problems. Some of the reasons for the extensive use of statistical learning methods are their flexibility, good predictive performance and ability to capture complex relationships among variables. In recent years, several computational tools to automate the workflow of the implementation of statistical learning models have been proposed, such as \texttt{caret} \citep{caret}, \texttt{h2o} \citep{h2o} and \texttt{tidymodels} \citep{tidymodels}, among others. These tools make it possible to train and tune many different algorithms, and the best one can be chosen based on a selected performance measure in a standardised way, reducing some common implementation errors. At the same time, models generated by the automatic workflow typically exhibit very good predictive performance; however, explaining the model’s decisions remains challenging.
These tools make more evident the inability to explain key aspects of the problem under consideration of statistical learning methods. Fortunately, there is a growing amount of research focused on methods for interpreting black box models. Broadly speaking, \textit{interpretability} is the degree to which a human can understand decisions or predictions from a statistical method \citep{miller2019explanation}. Having interpretable methods can be useful for detecting bias, understanding model errors, improving the model performance and understanding hidden relationships discovered by the algorithm, among other things. These reasons are relevant even when the objective is purely predictive. 

Interpretable methods can be divided into model-specific and model-agnostic methods. Model-specific methods are intrinsically interpretable based on model characteristics, such as regression coefficients. In this case, since the interpretation is intrinsic to the model, it can be difficult to compare different models. On the other hand, model-agnostic methods are used after model fitting; they apply techniques that make it possible to analyse the results of any subsequent model to be trained. A review of the different methods for improving interpretability can be found in \cite{molnar2020} or \cite{maksymiuk2020landscape}.  
In particular, this paper is mainly concerned with the individual conditional expectation plot (ICE-plot), which was proposed by \cite{goldstein2015peeking}. ICE curves are used to visualise the relationship between a response variable and a specific feature for each individual observation. The ICE-plot might help to represent heterogeneous effects in a problem but it may not work well in big data scenarios.  

The real estate market has a key role in the economic activity of a nation and plays a central role in economic and financial crises \citep{mooya2016standard}. The global financial crisis of 2007--2008 pointed out how the real estate market can create massive financial instability. This financial market differs from other markets due to property value heterogeneity. Understanding the spatial variability of property prices is a relevant economic problem for many financial and government organisations. An adequate model for property valuation is an important tool in the decision-making process in the public and private sectors \citep{osland2010,case2004}. 

Several statistical methods have been used to identify significant patterns in home pricing, from traditional hedonic models \citep{rosen1974} to advanced methods that include spatial dependency and non-linear relationships. Many proposed models assume that the property price can be decomposed linearly as the sum of its determinants. These models are easy to explain and to use to obtain the joint impact of the variables on the price. However, they are also often not as good with respect to the predictive performance, because the relationships that can be learned are very restricted and generally oversimplify reality. For these reasons, the use of statistical learning methods and non-linear models has grown significantly over the few last years in the real estate industry  \citep{limsombunchai2004, yoo2012,  park2015, goyeneche2017}. Although statistical learning methods have proven useful in real estate modelling for predictive purposes, the lack of transparency and interpretation limits their use.

Usually, in these models, two groups of explanatory factors are considered. In the first place, spatial information relative to the location based on the geographical coordinates of the real estate is considered. These variables are known to be key determinants of a piece of real estate's value \citep{kiel2008}. The neighbourhood, general service access, value of nearby properties, distance to special points of interest (downtown, a coast, etc.) and crime rate are some examples of factors that have a large impact on the property price. Additionally, a complementary set of explanatory variables includes features such as the number of bedrooms, number of bathrooms, and total area of the property; these are known in the property literature as hedonic variables.\citep{sirmans2005}. 

In problems in which the data contain a spatial structure, this structure can be used to improve the interpretability of statistical learning models in a natural way. The main goal of this paper is to combine interpretable statistical learning methods, specifically ICE curves, with spatial information. An extension of the ICE-plot is proposed that takes into account spatial restrictions to group the ICE curves based on the hierarchical clustering algorithm with spatial restrictions proposed in \cite{chavent2018} combined with a Sobolev distance function that considers the distance between ICE curves. Spatial ICE curves make it easier to interpret a model's results in problems in which the number of observations is large and the spatial information is relevant.

The paper is structured as follows. In  Section \ref{section:2}, we introduce the interpretable machine learning methods that we use in this paper, and we present the so-called spatial ICE curves. In Section \ref{section:appl}, the results are presented. We start the section with a description of the real estate data in Montevideo (Uruguay) that were used. Using the data, several statistical learning models were trained, and the main interpretable methods were applied. Finally, we compare the results with the spatial ICE (SpICE) curves proposed. Some final remarks and further work are discussed in Section \ref{section:4}.

\section{ICE curves in spatial problems} \label{section:2}

\subsection{Basic interpretability problem}
Let us consider a supervised problem, such that the goal of statistical learning models is to approximate 
\[
\mathbb{E}(Y|X=x) = f(x) \approx \hat f(x),
\]
where $X=(X_1,X_2,\dots, X_q)$ is a vector of $q$-variables, $Y$ is the response variable and $\hat f$ is the fitted model that predicts the scalar $Y$ as a function of $X$. In this context, one goal of interpretable methods is to characterise the dependence of the `main effects' on $f(x)$ for each explanatory variable. It is also possible to analyse the `low-order' dependence between pairs of variables. 

Let us assume that the main goal is to understand the effect of a set of explanatory variables denoted as $X_S$ on the response variable in a model-agnostic way ($S \in \{1,2,\dots,q\}$, and denote by the subset $C$ the complement of $S$). One of the earliest methods for this is the partial dependence plot (PD-plot) \citep{friedman2001}. The PD-plot describes the change of the response variable in a model as a function of the marginal effect of one or more variables (subset $X_S$) when the effects of the other explanatory variables (complementary subset $X_C$) are averaged. 

The main advantages of the PD-plot are that its estimation is very intuitive and that it presents a causal interpretation of the results of any learning model \citep{zhao2021causal}. The main drawbacks are that it hides heterogeneous effects, it might rely on an unrealistic set of observations and it is computationally expensive. Alternatives to the PD-plot have been proposed to overcome its problems, such as the accumulated local effects plot (ALE-plot) \citep{apley2020visualizing} and the individual conditional expectation plot (ICE-plot) \citep{goldstein2015peeking}.

\subsection{Individual conditional expectation plot (ICE-plot)} \label{sub:ICE}

The ICE-plot is proposed as an extension of the PD-plot to visualise the dependence of the prediction on a feature for each sample separately, with one curve per observation. The method attempts to capture the dependency of the response variable on a set of variables, allowing heterogeneous effects.

For each observation, the curve $\hat f_{S}^{(i)}(x_S)$ is obtained by varying $x_S$ in the function $\hat f(x_S,x_C^{(i)})$ while the variables $x_C^{(i)}$ remain constant. That is to say,
\begin{equation}
 \hat f_{S}^{(i)}(x_S)= \hat f(x_S, x_C^{(i)}). 
\end{equation}

It is worth noting that averaging the ICE curves corresponds to the definition of the PD-plot. 
%\[
%\hat f_{S,PD}(x_S) = \frac{1}{N}\sum_{i = 1}^ N \hat f(x_s, x_C^{(i)});
%\]
Thus, it is possible to interpret the ICE curves similarly to the classic PD-plot, but it is also possible to pick interactions when visualising the $N$ plots (something that vanishes when all ICE curves are averaged into one curve). A visual example is added to illustrate the ICE interpretation. Figure \ref{fig-3ice} shows ICE curves corresponding to individual observations from the dataset described in Section \ref{section:appl}; only three curves are presented to simplify the example. Each curve examines the association between the price per square metre and the corresponding total area of the apartment. Having individual curves for each observation makes it possible to visualise potential heterogeneous effects; in particular, the solid ICE line suggests a small effect at around $e^{3.5}$ square metres, while the two dashed ICE curves show a larger negative impact, with a pattern closer to linearity. 

\begin{figure}[htpb]
    \centering
    \includegraphics[scale=.5]{figures/fig-3ice-exmp.pdf}
    \caption{Individual ICE curves, where each line represents one selected observation.}
    \label{fig-3ice} 
\end{figure}
%\includegraphics[trim=0cm 4cm 0cm 4cm, clip, scale=1]

\subsection{Spatial ICE (SpICE) curves} 
\label{seccD}

The interpretation of ICE curves can be challenging when a large number of observations is available. A solution based on clustering ICE curves, which takes into account two sources of information, is proposed. The main source of information corresponds to the covariate effect profile described by the ICE curve itself. This is relevant to maintaining the original goal of the individual ICE curves. Additionally, when observations contain individual spatial information, location information can be used to improve the interpretability of ICE curves. Spatial data is data where the location of the measurements is a key element. This often includes data that references a specific geographical area. In this paper, SpICE refers to clustering ICE curves with spatial contiguity constraints.

In order to construct clusters of ICE curves, it is necessary to define a distance or dissimilarity measure between functions. There are many approaches that can be used to solve this problem; a typically used metric for functional clustering is the L2 distance: 
\[ d_{L2}(i, j) = \sqrt{ \int |\hat{f}_{S}^{(i)}(x) - \hat{f}_{S}^{(j)}(x)|^2dx}, \] 
where $d_{L2}(i, j)$ represents the L2 distance between ICE curve $i$ and ICE curve $j$. In Appendix \ref{toyexample}, a model example in which the L2 distance is not adequate to distinguish ICE curves with different predictor effects profiles is presented. Figure \ref{iceej} presents two scenarios in which $d_{L2}(\cdot,\cdot)$ produces the same value but the covariate effect is not the same. 

Moreover, \cite{hitchcock2015clustering} point out that in many applications, information about the derivatives of the function is even more relevant than the function itself.  In these cases, one approach could be to use the same L2 distance on the derivative functions. However, in clustering ICE curves, the key idea is to group observations with similar predictor effects on the response; thus, considering both the level and variation is important. For these reasons, it is appropriate to consider a distance that takes into account not only the value of the function but also its growth/decrease. Therefore, for the ICE function $\hat{f}_{S}^{(i)}(x_S)$, it is possible to use the Sobolev $W^{1,2}(\mathbb{R})$ distance \citep{sobolev}:  
\begin{equation}
\label{sobodis}
d_{Sob}(i, j) = \sqrt{ \int |\hat{f}_{S}^{(i)}(x) - \hat{f}_{S}^{(j)}(x)|^2dx + \int |\hat{f^{'}}_{S}^{(i)}(x) - \hat{f^{'}}_{S}^{(j)}(x)|^2dx}.    
\end{equation}  

The Sobolev distance has recently been used in some functional data applications as a way to highlight complex functional patterns. \cite{cremona2023probabilistic} and \cite{ehsani2020robust} report good performances when this type of distance is used in different statistical applications. Particularly in clustering functional data, using the Sobolev distance might improve the clustering solution. In Appendix \ref{simstudy}, results from a small simulation study are presented.  A classical functional data example \citep{hitchcock2007effect} is used to compare the proportions of correctly matched pairs of curves when the L2 or Sobolev distance is used. The Sobolev distance shows better results in at least 95\% of the 500 replications.   

However, some statistical learning models (such as tree-based methods) produce nondifferentiable predictor functions; thus, the ICE curves for these models inherit the nondifferentiability property, which makes the computation of $d_{Sob}(,)$ impossible. One way to overcome this issue and obtain estimates in a $C^1$ space is to consider the curve $\widetilde{f}_{S,ICE}^{(i)}(x_S)$, which is the convolution of the function  $\hat{f}(x_S, x_C^{(i)})$  with a Gaussian kernel. That is, 
\begin{equation}\label{}
 \widetilde{f}_{S}^{(i)}(x_S) := \ \hat{f}(x_S, x_C^{(i)}) \ast K_h(x_S),
\end{equation}
where $K_h$ is the Gaussian kernel, $h$ is the smoothing parameter and $\ast$ is the convolution operation. It is worth noting that this operation can be thought of as a pre-smoothing of the functional data, which has been shown to be beneficial in the cluster analysis of functions \citep{hitchcock2007effect}. Additionally, when the true $f(x)$ is a smooth function, the above transformation of the ICE curve (the convolution) will not distort the ICE estimator in a relevant way. 

When considering ICE curves, each curve is associated with a specific observation. In applications in which the location is a key element, i.e. spatial problems, it is possible to link such curves with the location information of the corresponding observation. Combining these two sources of information might improve the interpretability of ICE curves. 

\cite{chavent2018} propose an algorithm to construct clusters of multivariate data with spatial restrictions of contiguity. 
Let us consider a partition $\mathcal{P}_K = (C_1 ,\ldots , C_K )$ into $K$ clusters.  Using two dissimilarity matrices, $D_0$ and $D_1$, and a mixing parameter $\alpha \in [0,1]$, the pseudo-inertia in cluster $k$ can be defined as 
$$I_{\alpha} \left( C^{\alpha} _k \right)=  (1-\alpha) \sum_{i \in C^{\alpha}_k} \sum_{j \in C^{\alpha}_k} d^2_{0,ij}+\alpha \sum_{i \in C^{\alpha}_k} \sum_{j \in C^{\alpha}_k}d^2_{1,ij},$$
where $d_{0,ij}$ and $d_{1,ij}$ represent dissimilarities between observations $i$ and $j$ in $D_0$ and $D_1$, respectively. Then, a Ward-like method \citep{ward1963hierarchical} is used for the construction of the clusters, and a data-driven procedure to assist in the choice of the $\alpha$ value is proposed. 

In this paper, this clustering algorithm is adapted to cluster ICE curves (functional data). Mainly, the dissimilarity matrix $D_0$ represents distances among ICE curves instead of multivariate data. The Sobolev distance is used to compute dissimilarities among pre-smoothed ICE curves and to construct the $D_0$ matrix, i.e $d_{0,ij} = d_{sob}(i, j)$ (see Equation \eqref{sobodis}). As mentioned earlier, this distance takes into account the covariate effect profile in terms of both the level and variation and could be beneficial for the clustering solution. In addition to ICE information, the geographical distance between 
observations is used for the $D_1$ matrix. The \texttt{SpICE} R package implements the computation and visualisation of SpICE curves; it is available at  https://github.com/natydasilva/SpICE. 


\section{Application results}
\label{section:appl}

\subsection{Dataset: Apartment prices in Montevideo}
The data in this paper are from an eCommerce platform called \href{https://www.mercadolibre.com.uy)}{Mercado Libre}, where apartments and houses are offered for sale and for rent. The complete dataset contains asking price information for properties in Montevideo, the capital city of Uruguay, from February 2018 to January 2019 \citep{Picardo}. There are 92,832 observations; however, only apartment data will be considered for the analysis, which results in 70,817 observations concerning apartments for sale in Montevideo.

 Figure \ref{mapa} shows the distribution of the asking price for apartments in Montevideo, where each dot represents one apartment for sale and the colour represents the asking price of the apartment per square metre in US dollars. Figure \ref{mapa} suggests that locations close to the coast are associated with a higher price range. The asking price ranges from 541 to 5,000 US dollars per square metre in Montevideo, and the median value is close to 2,500 US dollars.  
 \begin{figure}[hbpt]
\centering
\includegraphics[width=1\linewidth]{figures/mapa_preciom2.png}
\caption{Distribution of the asking price for apartments in Montevideo. Each dot represents one apartment, and the colour represents the asking price offered by the seller in US dollars per square metre. 
\label{mapa} }
\end{figure}

\begin{table}[hbpt]
	\centering
	\caption{Variable description\label{tabvar} }
\scriptsize
\begin{tabular}{p{2.4cm}p{7cm}}
		\hline \\[-1.8ex] 
\textbf{Variable} &  	\textbf{Description} \\
		\hline \\[-1.8ex] 	\hline \\[-1.8ex] 

\texttt{lpricem2}  & Log of asking price in US dollars per square metre. \\ \hline
  
\texttt{amenities}  & Total number of amenities present in the apartment.\\
\texttt{bedroom}  & Bedroom quantity. Reduced to values between 0 (studio) and 3. \\
\texttt{bathroom}  & Bathroom quantity. Reduced to values between 1 and 3.\\
\texttt{elevators}  & Elevator quantity. Reduced to values between 0 and 2. \\
\texttt{condition}  & Property condition (new/used).  Properties that were less than 1 year old were marked as `new'.\\
\texttt{expenses}  &  Numerical value representing monthly expenses in Uruguayan pesos (local currency).\\
\texttt{garage}  & Whether or not there is a garage. Reduced to values between 0 (`No') and 1 (`Yes').\\ 
\texttt{ldistance\_beach}  & Minimum distance (Euclidean)  between  the property   and the beach (on a log scale). \\
\texttt{larea\_apt}  & Log of the apartment area in square metres. Values over 2,000 or under 9 square metres were removed from the data. \\
\texttt{neighborhoodgr}  &  Montevideo neighbourhoods grouped by proximity in 12 regions. \\ \hline
\texttt{lat} & Property latitude coordinate. \\
\texttt{long}  & Property longitude coordinate. \\
	\hline \\[-1.8ex] 	\hline \\[-1.8ex]  
	\end{tabular}
\end{table}

The complete dataset contains 116 explanatory variables representing apartment features commonly used in real estate modelling. With the available information, an additional two explanatory variables were created. First, a variable indicating the distance from the apartment to the beach was computed since it appears to be a relevant feature based on data exploration (see Figure \ref{mapa}). Second, many of the variables indicate the presence/absence of one specific amenity; thus, a variable indicating the total number of amenities present in an apartment was computed. 

A reduced number of variables were selected to improve the data quality of the complete dataset. Table \ref{tabvar} presents the selected variables, indicating the name of each variable in the dataset and a brief description. All the models were fitted using the natural logarithm of the  price per square metre (\texttt{lpricem2}) as the response variable. Finally, the geographical coordinates of each property were not used as explanatory variables in the models but were used as additional information to improve interpretability using the SpICE curves described earlier.
 
\subsection{Predictive models}
\label{modelo} 

Several models were trained %to estimate $f(x_i)$ 
using the automatic machine learning (\texttt{autoML}) procedure from the \texttt{h2o} R package \citep{h2o} to predict the apartment  price as an alternative to classical methods. \texttt{autoML} estimates well-tuned models in four families: penalised linear models (glm), random forest (drf), extreme gradient boosting (xgboost) and fully connected multi-layer artificial neural networks (deeplearning). Additionally,  stacked ensembles (stackedensemble) of individual models are trained; this includes the combination of all the models and ensembles using subsets of trained models. In the rest of the paper, the best model of each family and the best-performing stacked ensemble are used. 

Table \ref{comparo} shows the  performance measures for the selected predictive models.  The root mean square error (RMSE) and the $R^2$ value are used to evaluate the model performance. These values are computed with the response variable in logs (as this was done for the training of every model). Additionally, the mean absolute error (MAEo) and mean absolute percentage error (MAPEo) are both computed on the original scale of the response variable, so they have units of dollars per square metre.  The four measures are computed using a testing dataset different from the training samples (2/3 training and 1/3 testing). 

\input{figures/table-performance}
In terms of the predictive performance, the stacked ensemble, random forest and xgboost algorithms show similar performances; they are somewhat better than the deep learning method or the penalised linear model. It is worth noting that the stacked model combines seven tree-based models. The best model obtained an average error in the asking price of \$243 per square metre, or around 9.6\% of the observed price.  %The variability explained by  every model is between $0.74$ and $0.90$. 

\subsection{Variable importance measures}
\label{sec:varimp} 
The first approach to interpreting model results for statistical learning methods is to compute variable importance measures. 
The variable importance measures were scaled so that the most important variable in each model has a value of 1, which simplifies the model comparison.  In Figure \ref{fig-imp}, we show  the results  by model for the variable importance. The ordering of the predictors is similar in all models. The apartment area (\texttt{larea\_apt}) and the neighbourhood (\texttt{neighborhoodgr}) are the two most important variables for predicting the apartment price. All the tree-based methods (drf, xgboost and stackedensemble) show a third relevant variable, which is the distance from the apartment to the beach (\texttt{ldistance\_beach}). The rest of the predictor variables are not relevant for prediction.  

\begin{figure}[htpb]
    \centering
    %\includegraphics[trim=0cm 4cm 0cm 4cm, clip, scale=1]{figures/fig-importance.pdf}
    \includegraphics[scale=.9]{figures/fig-importance.pdf}
    \caption{Variable importance. Each panel represents a model, and the y-axis shows the variables included in all the models. The bar length represents the scaled variable importance measure used to predict the apartment price.}
    \label{fig-imp}
\end{figure}

\subsection{Partial effect of apartment area} \label{sub:resultados_pdp}
In subsection \ref{sec:varimp}, the variable importance was presented; these results provide a ranking of variables according to their relevance for predicting the response variable.  However, they do not provide information on the effect that the individual variables have on the response variable. In order to characterise the average effect of apartment features on the price, the PD-plot and the ALE-plot  are estimated. The algorithms used for this estimation are from the R packages \texttt{pdp} \citep{pdp} and \texttt{ALEPlot} \citep{aleplot}. 
%interpretable methods described in Section \ref{section:2} are computed. Computations of the PD-plot, ALE-plot and ICE curves are done using R packages \texttt{iml} \citep{iml} and \texttt{ALEPlot} \citep{aleplot}. 
\begin{figure}[hbpt]
    \centering
    %\includegraphics[trim=0cm 6cm 0cm 6cm, clip, scale=1]{figures/fig-efecto-lsub.pdf}
    \includegraphics[scale=.9]{figures/fig-efecto-lsub.pdf}
    \caption{Effect of the \texttt{larea\_apt} variable in different models. Each panel corresponds to a predictive model and the colour represents the interpretable method (ALE-plot or PD-plot).}
    \label{fig-pdpalesup}
\end{figure}

The apartment area (\texttt{larea\_apt}) is the most relevant feature in every model. The effect of this variable on the response is described with the PD-plots and ALE-plots shown in Figure \ref{fig-pdpalesup}, where each panel corresponds to a model. A negative effect is suggested by these plots, with a similar effect in every model. Especially, this is  true  in the middle of the range for the apartment area, where most of the observed samples lie. Some differences can be seen in very large or very small apartments, where random forest shows smaller effects on prices. 

Specifically, Figure \ref{fig-pdpalesup} shows the average effects of the variable \texttt{larea\_apt}  in the sample. In a scenario in which the impact of an explanatory variable on the response presents heterogeneity among the observations, the PD-plot and ALE-plot methods hide the variability of effects. An alternative method for interpretability that can be used to tackle this issue is the ICE-plot. The value of visualising the individual curves that compose the PD-plot lies in exploring other patterns in the effects, rather than just the mean value.  

When big datasets are involved, a major disadvantage of ICE-plots is overplotting, which makes it difficult to answer  anything. This is especially relevant for ICE curves since their main purpose is to look for heterogeneity patterns in the predictor effect. The use of graphical solutions such as transparency or 2-dimensional histograms is not suitable for plotting lines.  In the \texttt{h2o} implementation (\cite{hall2017}), a few ICE curves for decile values (in the observed response) are displayed, so no matter how large the sample data are, only ten ICE curves are displayed, resulting in an oversimplified plot. An alternative is to plot a stratified sample of curves. This method can be combined with line transparency to allow one to reduce overplotting while making it possible to see the different patterns of the ICE curves in the data. Figure \ref{fig-icesupRF} shows the individual conditional expectation plot for the apartment area (\texttt{larea\_apt}) for the  random forest model. The other predictive models shows similar results, as can be seen in Figure \ref{fig-icesup} in Appendix \ref{app:figsup}. 

\begin{figure} 
    \centering
    \includegraphics[scale=0.5]{figures/fig-ice-sup-decRF.pdf}
    \caption{ICE-plot for the \texttt{larea\_apt} variable. Each panel corresponds to a predictive model. Five thousand randomly stratified selected curves are shown.}
    \label{fig-icesupRF}
\end{figure}

The results illustrate the negative effect of the variable \texttt{larea\_apt} in each individual curve. However, it is relevant to note that the effect  presents  heterogeneity for the different properties. Figure \ref{fig-icesupRF} suggests that for properties with a high predicted price, the apartment area presents a small and linear effect, while cheap properties show a non-linear and larger impact of the apartment area. 

\subsection{SpICE curve effects} \label{results-ice}
Finally, to explore the connections among ICE curves and the geographical locations of properties, the SpICE curve results are presented.  
A range of three to five clusters is considered; for each value, an optimal value for the $\alpha$ parameter is determined as a compromise between the internal homogeneity of the clusters in terms of the ICE curves and the geographical information, approximately. The results are shown in Figure \ref{alpfaoptimo} in Appendix \ref{app:figsup}, using four groups and $\alpha = 0.5$.

Figure \ref{fig-spice} show the results of the geographically constrained clusters; an apartment for sale in Montevideo is represented as a point on the map that is linked to an ICE curve in the bottom panel. The colours of the points and curves indicate the cluster that they belong to. The cluster locations suggest a layout in the north-west/south-east direction, similar to the price gradient present in the data (see Figure \ref{mapa}). The apartments in the west and on the north-west side (green cluster) correspond to low-income neighbourhoods, while the east side of the city (red cluster) represents the zone with the highest income in Montevideo. 

Focusing on the SpICE curves presented in Figure \ref{fig-spice}, there are distinct patterns in the relationship between the apartment area and the asking price per square metre. Across all clusters, there is a negative relationship between the price per square metre and the apartment area. However, the effect of the apartment area on the price per square metre differs between the green and blue clusters (associated with medium- and low-income neighbourhoods) and the red and violet clusters (associated with high-income neighbourhoods). In the red and violet clusters, an increase in the apartment area results in a smaller decrease in the price per square metre compared to the green and blue clusters. This suggests that, in high-income neighbourhoods, the total apartment price (not the price per square metre) is very sensitive to changes in the apartment area. Conversely, in low-income neighbourhoods, the total apartment price is inelastic to changes in the apartment area. Consequently, additional square metres in the apartment have no impact on the total apartment price, leading to a decline in the price per square metre. %within the low and medium-income neighborhoods.


\begin{figure} 
    \centering
    %{figures/fig-4grupos.pdf}
    \includegraphics[scale=0.9]{figures/fig-4grupos-alpha5.pdf}
    \caption{SpICE curves and geographical locations of clusters.}
    \label{fig-spice}
\end{figure}



\section{Discussion} \label{section:4}

In this paper, interpretable methods for measuring heterogeneous covariate effects for black box models, functional data clustering and spatial information were combined to improve interpretability in spatial applications. In particular, geographically constrained clusters of ICE curves called SpICE curves were constructed, combining two sources of information.  On the one hand, the covariate effect profile is considered by using the Sobolev distance as a dissimilarity measure for the ICE curves. The Sobolev distance is useful from an interpretability point of view since it summarises the main characteristics of the covariate effect (level and variation). On the other hand, the location information of observations is used to obtain geographically contiguous clusters. 

Similarly to the ICE-plot, the SpICE curves can show the heterogeneous effects of a predictor variable in a black box model. SpICE curves are easier to work with in big data applications since it is possible to interpret each cluster instead of each individual curve. In specific problems, the spatial contiguity of the cluster provides more interpretable information associated with other relevant aspects that may not be present in the ICE curves. 

In the motivating example, SpICE curves were constructed to profile the effect of the total area of a property on the  price per square metre in Montevideo apartments. Five statistical learning methods were selected from a list of fitted models using \texttt{h2o} with the \texttt{autoML} procedure based on the predictive performance. The predictor variable `total area of the apartment' was (unsurprisingly) the most important feature in  all models. Then, using a fitted model with the random forest algorithm, the ICE-plot and SpICE curves for each apartment were computed. The spatial information of the properties was combined with ICE curves to gain interpretability. Four property clusters were selected, based on a distance that combines the functional distance between ICE curves and the geographical distance between apartment coordinates.  The blue and green clusters mainly represent properties located in medium- or low-income zones in Montevideo, and these clusters present a large, negative, non-linear effect of the apartment area on the price per square metre. On the other hand, the red and violet clusters mainly correspond to high-income neighbourhoods, and they show a small, close-to-linear effect of the property area. 

There are several aspects of this paper that could be explored in future work. First, the choice of the distance between the ICE curves could be improved; specifically, it is relevant to analyse more deeply in which scenarios the Sobolev distance results in a better clustering solution for functional data.  Second, instead of constructing fixed clusters of ICE curves, it is possible to consider the local average of the ICE curves in a nearest neighbours fashion, where the distance used to determine the neighbours could combine the structure of the ICE curves and the geographical distance. Finally, a summary of the clustered ICE curves could be based on the ALE-plot instead of the average of the clustered ICE curves. 

\section{ Supplementary material }
This article was written with the R packages \texttt{knitr} \citep{knitr}, \texttt{ggplot2} \citep{hadley:2016}, \texttt{leaflet} \citep{leaflet}, \texttt{tidyverse} \citep{tidyverse}, \texttt{h2o} \citep{h2o}, \texttt{ClustGeo} \citep{clustGeo} and
 \texttt{KernSmooth} \citep{KernSmooth}. The files needed to reproduce the article and the results are available at https://github.com/natydasilva/SpICE\_COST. Additionally, the \texttt{SpICE} R package implements the computation and visualisation of SpICE curves proposed in this paper and  it is available at  https://github.com/natydasilva/SpICE. 





% \renewcommand{\refname}{References }
\bibliographystyle{apa}
\bibliography{bibcsic.bib}




\appendix

\section{Minimal example \label{toyexample}}  
This section presents a model example in which the L2 distance is not a good choice for finding dissimilarities among ICE curves. Let us assume the regression model 
\[\begin{array}{cl}
  Y_i   &  = f(X_{1,i}, X_{2,i}) + \epsilon_i  \\
     & = -C(1-X_{1,i})X_{1,i} + \frac{1}{2}X_{2,i}(2- X_{1,i}-X^2_{1,i}) +X_{2,i}^{1+X_{1,i} \1{ \{X_{1,i}\geq 0\} } }  + \epsilon_i,
\end{array}\]
where $X_1 \sim U(-1,1)$, $X_2\sim U(0,2)$ and $\epsilon_i$ represents white noise with a standard deviation $\sigma>0$. When the true $f()$ function is known, the ICE curves at three chosen points are $f(-1,x2)= 2C+2x_2$, $f(0,x2)=2x_2$ and $f(1,x2)=x_2^2$. If $C= -\sqrt{ \frac{\int_0^2(x^2-2x)^2 dx}{8}}$, then 
$$d_2\left( f(-1,\cdot), f(0,\cdot)\right)= d_2\left( f(1,\cdot), f(0,\cdot)\right).$$

Figure \ref{iceej} shows these three ICE curves in two panels. The left panel (case 1) contains $f(-1,\cdot)$ and $f(0,\cdot)$, two parallel linear functions representing the same effect of $X2$ on the response variable. Meanwhile, the right panel (case 2) shows the ICE curves for $f(1,\cdot)$ and $f(0,\cdot)$; here, the two curves represent different effects of $X2$. 

\begin{figure}[hbpt]
\centering
%\includegraphics[height=4cm,  width=1\linewidth]{figures/fig-toyexample.pdf}
\includegraphics[trim=0cm 4cm 0cm 3cm, clip, scale=.7]{figures/fig-toyexample.pdf}
\caption{ICE curves from the model example. \label{iceej} }
\end{figure}
From an interpretability point of view, it is important to differentiate between the two cases plotted in Figure \ref{iceej}; however, the L2 distance is the same in these two cases. 

\section{Simulation study \label{simstudy}}

A small simulation study comparing the performances of the L2 and Sobolev distances in clustering functional data is performed, following the design choices proposed in \cite{hitchcock2007effect}. 
\begin{figure}[hbpt]
\centering
%\includegraphics[height=4cm,  width=1\linewidth]{figures/fig-toyexample.pdf}
\includegraphics[trim=0cm 4cm 0cm 3cm, clip, scale=.6]{figures/fig-simstudy1.pdf}
\caption{True mean functions. \label{meanfun} }
\end{figure}

Functional data are simulated with a four-cluster structure, based on four functions that represent the mean of each group. Figure \ref{meanfun} shows the true mean functions, $\mu_g(x), \; x \in \left[0, 5\right]$, where $g=1,2,3,4$ indicates the cluster. According to \cite{hitchcock2007effect}, the $\mu_g(\cdot)$ \textit{`... were intentionally chosen to be similar enough to provide a good test for the clustering methods that attempted to group the curves into the correct clustering structure, yet different enough that they represented four clearly distinct
processes.'}

Synthetic data are obtained by adding random noise to $\mu_g(x)$:  
\[ y_{ig} = \mu_g(x_i) + u_g + \epsilon_{ig}, \]
where $u_g \sim N(0, 1)$ and $\epsilon_{ig} \sim N(0, 0.1^2)$. Figure \ref{dataset} shows a simulated dataset, consisting of $N=40$ curves (10 curves from each group) represented in a discretised form, with $n=50$ values per curve, equally spaced in $\left[0, 5\right]$.  

\begin{figure}[hbpt]
\centering
\includegraphics[trim=0cm 4cm 0cm 3cm, clip, scale=.6]{figures/fig-simstudy2.pdf}
\caption{ A simulated dataset. \label{dataset} }
\end{figure} 

The clusters are estimated using the k-medoids method with the \texttt{pam()} function in \texttt{R}, with pre-smoothed data as the main input. Two cluster solutions are obtained for each simulated dataset, 
using the L2 and Sobolev distances as the dissimilarity function. The cluster solution performance is evaluated based on the proportion of pairs of curves that are correctly matched in the same group. The process is replicated $500$ times. 

Table \ref{sim-results} presents results from the 500 replications. Using the Sobolev distance provides a larger proportion of correctly matched pairs on average; additionally, the 5\% quantile of the ratio is slightly larger than 1, so the Sobolev distance resulted in a better performance for at least 95\% of the simulations. 

\input{figures/tab-simstudy-res}

\section{Supplementary figures}
\label{app:figsup}

\begin{figure}[hbpt]
    \centering
    \includegraphics[scale=0.6]{figures/fig-ice-sup-dec.pdf}
    \caption{ICE-plot for the \textit{log apartment area} variable. Each panel corresponds to a predictive model. Five thousand randomly stratified selected curves are shown.}
    \label{fig-icesup}
\end{figure}

\begin{figure}[hbpt] 
    \centering
    \includegraphics[trim=0 4cm 0 4cm, clip, scale=.6]{figures/fig-alphaoptimo.pdf}
    \caption{Optimal $\alpha$ for different groups.}
    \label{alpfaoptimo}
\end{figure}


\end{document}
